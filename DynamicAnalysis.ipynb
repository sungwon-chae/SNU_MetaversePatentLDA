{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 01. Dynamic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_only_file(directory, file_name, is_make_temp_dir = False):\n",
    "    \"\"\"파일명으로 생성\"\"\"\n",
    "    if is_make_temp_dir is True :\n",
    "        directory = mkdtemp()\n",
    "    if len(directory) >= 2 and not os.path.exists(directory) :\n",
    "        os.makedirs(directory)\n",
    "    return os.path.join(file_name)\n",
    "\n",
    "def make_path(directory, file_name, is_make_temp_dir = False):\n",
    "    \"\"\"디렉토리 + 파일명으로 경로 생성\"\"\"\n",
    "    if is_make_temp_dir is True :\n",
    "        directory = mkdtemp()\n",
    "    if len(directory) >= 2 and not os.path.exists(directory) :\n",
    "        os.makedirs(directory)\n",
    "    return os.path.join(directory, file_name)\n",
    "\n",
    "class Custom_Error(Exception) :\n",
    "    pass\n",
    "\n",
    "#%%\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "directory = 'C:/Users/chaesungwon/OneDrive - 서울대학교/csw/dtm_v4/input/patent/'\n",
    "df = pd.read_csv(directory + 'drug_side_effect.csv',encoding='latin_1')\n",
    "df['text'] = df['name'] + ' ' + df['Abstract']\n",
    "\n",
    "df['YEAR'] = [str(i)[:4] for i in df['YEAR']]\n",
    "docs = df[\"text\"].tolist()\n",
    "\n",
    "\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''    \n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "word_list = []\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    \n",
    "    doc = nlp(docs[idx])\n",
    "    \n",
    "    temp = []\n",
    "    for token in doc :\n",
    "        temp.append(token.lemma_.lower())\n",
    "    \n",
    "    word_list.append(temp)  # word_list insert\n",
    "    #word_list[idx] = word_list[idx].lower()  # Convert to lowercase.\n",
    "    #word_list[idx] = tokenizer.tokenize(word_list[idx])  # Split into words.\n",
    "\n",
    "rgxWord = re.compile('[a-zA-Z][-_a-zA-Z0-9.]*') \n",
    "\n",
    "# hoho\n",
    "word_list = [[token for token in doc if rgxWord.match(token)] for doc in word_list]\n",
    "\n",
    "# Remove words that are only one/two character.\n",
    "word_list = [[token for token in doc if len(token) > 2] for doc in word_list]\n",
    "\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "# do the nlp stuff\n",
    "word_list = [[token for (token, pos) in nltk.pos_tag(doc) if is_noun(pos)] for doc in word_list]\n",
    "\n",
    "# 스탑워즈 제거\n",
    "\n",
    "# stopwords_smart\n",
    "\n",
    "f = open('C:/Users/chaesungwon/OneDrive - 서울대학교/csw/dtm_v4/input/stopwords_smart.txt', mode='rt', encoding='utf-8')\n",
    "L = []\n",
    "\n",
    "for line in f:\n",
    "    L.append(line.strip())\n",
    "\n",
    "stop = L\n",
    "\n",
    "for i in stop:\n",
    "    temp_word = i\n",
    "    word_list = [[token for token in doc if not token == temp_word] for doc in word_list]\n",
    "    \n",
    "    \n",
    "# stopwords_tech\n",
    "\n",
    "f = open('C:/Users/chaesungwon/OneDrive - 서울대학교/csw/dtm_v4/input/stopwords_tech.txt', mode='rt', encoding='utf-8')\n",
    "L = []\n",
    "\n",
    "for line in f:\n",
    "    L.append(line.strip())\n",
    "\n",
    "stop = L\n",
    "\n",
    "for i in stop:\n",
    "    temp_word = i\n",
    "    word_list = [[token for token in doc if not token == temp_word] for doc in word_list]\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "from gensim.models import ldaseqmodel\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.matutils import hellinger\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.wrappers import DtmModel\n",
    "import numpy\n",
    "import os\n",
    "from datetime import datetime\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# path to dtm home folder\n",
    "dtm_home = os.environ.get('DTM_HOME', \"dtm-master\")\n",
    "# path to the binary. on my PC the executable file is dtm-master/bin/dtm\n",
    "dtm_path = os.path.join(dtm_home, 'bin', 'dtm') if dtm_home else None\n",
    "# you can also copy the path down directly. Change this variable to your DTM executable before running.\n",
    "dtm_path = \"C:/Users/chaesungwon/OneDrive - 서울대학교/csw/dtm_v4/input/dtm-master/bin/dtm-win64.exe\"\n",
    "\n",
    "#%%\n",
    "#1년단위\n",
    "try:\n",
    "    time_count = df['YEAR'].value_counts().sort_index()\n",
    "except:\n",
    "    time_count = df['YEAR'].value_counts()\n",
    "\n",
    "time_count = time_count.to_list()\n",
    "\n",
    "dictionary = Dictionary(word_list)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in word_list]\n",
    "num_topics = 20\n",
    "\n",
    "dtm_model = DtmModel(dtm_path, corpus, time_count, num_topics=num_topics, id2word=dictionary,\n",
    "                      initialize_lda=True, alpha=0.01, top_chain_var=0.05)\n",
    "\n",
    "# dtm_model.save(DIR_output + 'dtm_model5')\n",
    "\n",
    "DIR_output = 'C:/Users/chaesungwon/OneDrive - 서울대학교/csw/dtm_v4/output/'\n",
    "#%% 문헌 토픽 비율 라벨링\n",
    "\n",
    "topic_table = pd.DataFrame()\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    temp = pd.Series(dtm_model.gamma_[i])\n",
    "    topic_table = topic_table.append(temp, ignore_index=True)\n",
    "\n",
    "topic_table.columns = [\"topic_\" + repr(x) for x in range(len(topic_table.iloc[0, :]))]\n",
    "topic_df = pd.concat([topic_table, topic_table.idxmax(axis=1), topic_table.max(axis=1)], axis=1)\n",
    "length = len(topic_table.iloc[0, :])\n",
    "\n",
    "topic_df.columns.values[length] = \"topic_num\"\n",
    "topic_df.columns.values[length + 1] = \"max_prop\"\n",
    "topic_df['period'] = df['YEAR']\n",
    "\n",
    "after_docs = docs[:]\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    after_docs[idx] = ' '.join(word_list[idx])\n",
    "after_docs = pd.DataFrame(after_docs)\n",
    "\n",
    "app_num = df.YEAR.tolist()\n",
    "app_num = pd.DataFrame(app_num)\n",
    "df_after = pd.concat([after_docs, topic_df.loc[:, \"topic_num\"], topic_df.loc[:, \"max_prop\"], app_num], axis=1)\n",
    "df_after.columns = [\"word_list\", \"main_topic\", \"topic_prop\", \"period\"]\n",
    "#df_after = df_after[df_after.topic_prop >= 0.30]\n",
    "# df_after.to_csv(DIR_output + \"doc_to_topic.csv\", index=False)\n",
    "\n",
    "topic_df = topic_df.drop(['topic_num','max_prop'], axis = 1)\n",
    "\n",
    "import statistics\n",
    "\n",
    "def make_word_vector(word):\n",
    "\n",
    "    word = word\n",
    "    temp_data = {'word': word}\n",
    "    temp_set = list(set(df_after.loc[:, \"period\"]))\n",
    "    temp_set = repr(temp_set)\n",
    "    sorted(temp_set)\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    for j in range(len(set(df_after.loc[:, \"period\"]))):  # 기간 동안의\n",
    "        temp_period = temp_set[j]\n",
    "        temp_data[repr(count)] = 0\n",
    "        count += 1\n",
    "\n",
    "    temp_data = pd.DataFrame([temp_data])\n",
    "    temp_list = sorted(temp_data.iloc[:, 1:(len(temp_data.columns))], key= repr)\n",
    "    temp_list.insert(0, 'word')\n",
    "\n",
    "    temp_data = temp_data[temp_list]\n",
    "\n",
    "    return (temp_data)\n",
    "\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error : creating directory.\" + directory)\n",
    "\n",
    "\n",
    "def CAGR(x, last_col, first_col, num_periods):\n",
    "    \"\"\" Calculate compound growth rate on a row x of a pandas df\"\"\"\n",
    "    val_T = float(x[last_col]) + 0.00001\n",
    "    val_t = float(x[first_col]) + 0.00001\n",
    "    return ((val_T / val_t) ** (1. / num_periods) - 1) * 100\n",
    "            \n",
    "#%%\n",
    "\n",
    "for i in range(num_topics):\n",
    "\n",
    "    topic = i\n",
    "    resultlist = []\n",
    "\n",
    "    for j in range(len(set(df_after.loc[:, \"period\"]))):  # 기간 동안의\n",
    "        temp = dtm_model.show_topic(i, j)\n",
    "        resultlist.append(temp)\n",
    "\n",
    "    df = pd.DataFrame(resultlist)\n",
    "\n",
    "    dtw_df = make_word_vector('temp')\n",
    "\n",
    "    for j in range(len(df)):\n",
    "\n",
    "        for k in range(len(df.loc[j])):\n",
    "            \n",
    "            temp_word = df.loc[j][k][1]  # 단어 저장\n",
    "            \n",
    "            temp_list = dtw_df.loc[:, \"word\"].values.tolist()\n",
    "\n",
    "            if temp_word in temp_list:\n",
    "                dtw_df.loc[dtw_df.word == temp_word, repr(j + 1)] = df.loc[j][k][0]  # 확률저장\n",
    "            else:\n",
    "                temp_df = make_word_vector(temp_word)\n",
    "                temp_df.iloc[:, j+1] = df.loc[j][k][0]  # 확률저장\n",
    "                dtw_df = dtw_df.append(temp_df, ignore_index=True)\n",
    "\n",
    "    dtw_df = dtw_df.iloc[1:, :]\n",
    "    dtw_df.iloc[:, 1:].stack().idxmax()\n",
    "\n",
    "    max_row = dtw_df.iloc[:, 1:].stack().idxmax()[0]  # 가장 큰 값의 row\n",
    "    # max2_row = dtw_df.iloc[:,1:].stack().nlargest(2).index[1][0] # 2번째 큰 값의 row # 수정필요함\n",
    "\n",
    "    if (i >= 10):\n",
    "        DIR_temp = DIR_output + \"topic_\" + repr(i) + \"_\" + dtw_df.loc[max_row, \"word\"]\n",
    "        createFolder(DIR_temp)\n",
    "        DIR_temp = DIR_temp + \"/\" + \"topic_\" + repr(i) + \"_\" + dtw_df.loc[max_row, \"word\"]\n",
    "        dtw_df.to_csv(DIR_temp + \".csv\", header=False, index=False, encoding= \"utf-8\")\n",
    "    else:\n",
    "        DIR_temp = DIR_output + \"topic_0\" + repr(i) + \"_\" + dtw_df.loc[max_row, \"word\"]\n",
    "        createFolder(DIR_temp)\n",
    "        DIR_temp = DIR_temp + \"/\" + \"topic_0\" + repr(i) + \"_\" + dtw_df.loc[max_row, \"word\"]\n",
    "        dtw_df.to_csv(DIR_temp + \".csv\", header=False, index=False, encoding= \"utf-8\")\n",
    "\n",
    "\n",
    "#%% 토픽 변화 시각화\n",
    "import pandas as pd\n",
    "import string as str\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "output_name_set = []\n",
    "\n",
    "for f in [make_only_file(DIR_output, file) for file in os.listdir(path=DIR_output) if \"topic_\" in file]:\n",
    "    output_name_set.append(f)\n",
    "\n",
    "# temp_list1 = temp_df.sum(axis=0).sort_values(ascending=False)[0:5].index\n",
    "# temp_df1 = temp_df.loc[:, temp_list1]\n",
    "# temp_df1.plot(marker=\"v\")\n",
    "\n",
    "# 1. 상위 5개의 토픽 추출\n",
    "topicSize_df = topic_df.groupby(topic_df['period']).sum()\n",
    "\n",
    "\n",
    "temp_ = topicSize_df.sum(axis=0).sort_values(ascending=False)[0:5].index.tolist()\n",
    "temp_df1 = topicSize_df.loc[:, temp_]\n",
    "temp_df1.plot(marker=\"v\")\n",
    "plt.title(\"A_Head topic(top 5) trends\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.savefig(DIR_output + \"A_Head topic(top 5) trends.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close('all')\n",
    "\n",
    "\n",
    "# 2. 최근 주요 토픽 5\n",
    "temp2_ = topicSize_df.iloc[6,:].sort_values(ascending=False).index.tolist()\n",
    "temp2_ = [i for i in temp2_ if i not in temp_][:5]\n",
    "temp_df2 = topicSize_df.loc[:, temp2_]\n",
    "temp_df2.plot(marker=\"v\")\n",
    "plt.title(\"B_Recent issue topic(top 5) trends\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.savefig(DIR_output + \"B_Recent issue topic(top 5) trends.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close('all')\n",
    "\n",
    "#%%\n",
    "\n",
    "for i in range(len(output_name_set)):\n",
    "\n",
    "    DIR_temp = DIR_output + output_name_set[i] + \"/\"\n",
    "    dtw_df = pd.read_csv(DIR_temp + output_name_set[i] + \".csv\", header=None,  encoding = \"utf-8\")\n",
    "\n",
    "    dtw_df.index = dtw_df[0]\n",
    "    dtw_df = dtw_df.drop([0], axis=1)\n",
    "    dtw_df = dtw_df.loc[dtw_df[6] >= 0,:] #유의미한 키워드셋\n",
    "    temp_word_df = dtw_df\n",
    "    \n",
    "    # 탑 키워드 추출\n",
    "    temp_ = dtw_df.sum(axis = 1).sort_values(ascending=False).head(5).index.tolist() \n",
    "\n",
    "    # temp_df.high != 0\n",
    "    temp2_ = [] # 최근 주요 키워드 리스트 10\n",
    "    temp3_ = [] # 최근 급감 키워드 리스트 10\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=[\"keyword\", \"grow_ratio\", \"classification_result\"])  # 빈 결과\n",
    "\n",
    "    for j in range(len(temp_word_df.iloc[:, 0])):\n",
    "        \n",
    "        word_name = temp_word_df.index[j]\n",
    "        # temp_before = \n",
    "        temp_word_ = temp_word_df.iloc[j,:].tolist()\n",
    "        temp_recent = temp_word_[-1]\n",
    "        temp_before_ = temp_word_[:-2]\n",
    "        temp_before_ = [i for i in temp_before_ if (i > 0.005) ]\n",
    "        try : \n",
    "            temp_before = np.mean(temp_before_)\n",
    "        except : \n",
    "            temp_before = 0.00001\n",
    "        \n",
    "        grow_ratio = (temp_recent / temp_before -1)* 100 \n",
    "        \n",
    "        temp_result = [word_name,grow_ratio,\"normal\"]\n",
    "        result_df = result_df.append(pd.DataFrame([temp_result], columns=['keyword', \"grow_ratio\", 'classification_result']), ignore_index=True)\n",
    "    \n",
    "    temp2_ = result_df.loc[~result_df[\"keyword\"].isin(temp_) , :].sort_values(by = 'grow_ratio',ascending = False).head(5)['keyword'].tolist()\n",
    "    temp3_ = result_df.loc[~result_df[\"keyword\"].isin(temp_)  ,:].sort_values(by = 'grow_ratio',ascending = True).head(5)['keyword'].tolist()\n",
    "    \n",
    "    \n",
    "    result_df.loc[result_df[\"keyword\"].isin(temp_),\"classification_result\"] = 'head'\n",
    "    result_df.loc[result_df[\"keyword\"].isin(temp2_),\"classification_result\"] = 'issue'\n",
    "    result_df.loc[result_df[\"keyword\"].isin(temp3_),\"classification_result\"] = 'stagnant'\n",
    "    \n",
    "    result_df.to_csv(DIR_temp + \"keyword_results.csv\", index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    dtw_df = dtw_df.transpose()\n",
    "    temp_xtick = list(set(df_after.period))\n",
    "    temp_xtick.sort()\n",
    "    dtw_df.index = temp_xtick\n",
    "\n",
    "    try:\n",
    "        dtw_df[temp_].plot(marker=\">\")\n",
    "        plt.title(\"A.Head keyword trends for \" + output_name_set[i])\n",
    "        plt.xlabel(\"Period\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(DIR_temp + \"A.Head keyword trends for \" + output_name_set[i] + \".png\", bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close('all')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dtw_df[temp2_].plot(marker=\">\")\n",
    "        plt.title(\"B.Issue keyword trends for \" + output_name_set[i])\n",
    "        plt.xlabel(\"Period\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(DIR_temp + \"B.Issue keyword trends for \" + output_name_set[i] + \".png\",\n",
    "                    bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close('all')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dtw_df[temp3_].plot(marker=\">\")\n",
    "        plt.title(\"C.Stagnant keyword trends for \" + output_name_set[i])\n",
    "        plt.xlabel(\"Period\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(DIR_temp + \"C.Stagnant keyword trends for \" + output_name_set[i] + \".png\",\n",
    "                    bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close('all')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 02. dtm network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = topicSize_df.sum()\n",
    "node_df = node_df.reset_index()\n",
    "node_df.columns = ['Id','size']\n",
    "\n",
    "node_df[\"category\"] = ['application','application','application','technology',\n",
    "                         'market','application','technology','application',\n",
    "                         'market','market','technology','market',\n",
    "                         'application','application','application','technology',\n",
    "                         'application','application','technology','technology']\n",
    "\n",
    "node_df.to_csv(DIR_output + \"topic_node.csv\", index=False)\n",
    "#%%\n",
    "\n",
    "select = [x for x in topic_df.columns if x != \"period\"]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "col = ['source','target','year','weight','state']\n",
    "result_df = pd.DataFrame(columns = col)\n",
    "result_df['state'] = result_df['state'].astype('string')\n",
    "result_df['year'] = result_df['year'].astype('int')\n",
    "temp_df = pd.DataFrame(columns = col)\n",
    "# temp_df = temp_df.astype('string')\n",
    "\n",
    "temp_xtick = sorted(list(set(topic_df['period'])))\n",
    "temp_xtick = [int(i) for i in temp_xtick]\n",
    "\n",
    "for i in range(len(temp_xtick)) :\n",
    "    \n",
    "    period = temp_xtick[i]\n",
    "    \n",
    "    df_corr = pd.DataFrame() # Correlation matrix\n",
    "    df_p = pd.DataFrame()  # Matrix of p-values\n",
    "    df = topic_df.loc[topic_df[\"period\"] == period , select]\n",
    "    \n",
    "    # p , corr 연산\n",
    "    for x in df.columns:\n",
    "        for y in df.columns:\n",
    "            corr = stats.kendalltau(df[x], df[y])\n",
    "            df_corr.loc[x,y] = corr[0]\n",
    "            df_p.loc[x,y] = corr[1]\n",
    "    \n",
    "    # p 기준 투입 \n",
    "    col_names = df_p.columns.values\n",
    "    \n",
    "    for col, row in ((df_p <= 0.05) & (df_p > 0)).iteritems():\n",
    "        \n",
    "        topic_1 = col\n",
    "        \n",
    "        for topic_2 in col_names[row.values] :\n",
    "            \n",
    "            corr_value = df_corr.loc[topic_1,topic_2]\n",
    "            \n",
    "            if ((corr_value < 0.5) | (corr_value > 0.9)) : \n",
    "                continue\n",
    "            else :\n",
    "                topic_1_size = float(topicSize_df.loc[period,topic_1])\n",
    "                topic_2_size = float(topicSize_df.loc[period,topic_2])\n",
    "                \n",
    "                if topic_1_size >= topic_2_size :\n",
    "                    source = topic_1\n",
    "                    target = topic_2\n",
    "                else : \n",
    "                    source = topic_2\n",
    "                    target = topic_1\n",
    "                \n",
    "                temp1 = node_df.loc[node_df[\"Id\"] == source,\"category\"].values[0]\n",
    "                temp2 = node_df.loc[node_df[\"Id\"] == target,\"category\"].values[0]\n",
    "                \n",
    "                if temp1 == temp2 : state = \"inner\"\n",
    "                else : state= \"outer\"\n",
    "                    \n",
    "                temp_df = temp_df.append(pd.DataFrame({'source' : source,\n",
    "                                                       'target' : target,\n",
    "                                                       'year' : period,\n",
    "                                                       'weight' : 1.0,\n",
    "                                                       'state' : state},index = [0]), ignore_index = True)\n",
    "                \n",
    "    # 뉴 소스 및 타겟정보 추가\n",
    "    # temp_df = temp_df.astype('string')\n",
    "    \n",
    "    temp_df = temp_df.drop_duplicates([\"source\",\"target\",\"year\"]) # 중복제거\n",
    "    temp_df = temp_df.groupby([\"source\",\"target\",'state']).sum().reset_index()\n",
    "    temp_df[\"year\"] = period\n",
    "    # temp_df[\"\"] =\n",
    "    \n",
    "    \n",
    "    temp_before_df = result_df.loc[result_df[\"year\"] == (period -1), :]\n",
    "    # temp_df.loc[temp_df[\"\"],'state'] = \n",
    "    \n",
    "    # temp_df['state'] = temp\n",
    "    # temp_df = temp_df.astype('string')\n",
    "    \n",
    "    result_df = pd.concat([result_df,temp_df], ignore_index=True)\n",
    "    # result_df['state'] = result_df['state'].astype('string')\n",
    "    \n",
    "    \n",
    "#%%\n",
    "\n",
    "# result_df = result_df.drop_duplicates(\"corr\")\n",
    "result_df.to_csv(DIR_output + \"topic_edge.csv\", index=False)\n",
    "\n",
    "#%%\n",
    "print(result_df.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
