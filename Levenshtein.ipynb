{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Levenshtein\n",
      "  Downloading Levenshtein-0.18.1-cp37-cp37m-macosx_10_9_x86_64.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.0.1\n",
      "  Downloading rapidfuzz-2.0.11-cp37-cp37m-macosx_10_9_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting jarowinkler<1.1.0,>=1.0.2\n",
      "  Downloading jarowinkler-1.0.2-cp37-cp37m-macosx_10_9_x86_64.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jarowinkler, rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.18.1 jarowinkler-1.0.2 rapidfuzz-2.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-254cf6cdca2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jun  7 10:36:21 2022\n",
    "\n",
    "@author: tmlab\n",
    "\"\"\"\n",
    "\n",
    "from Levenshtein import distance as lev\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def wisdomain_prep(data) :\n",
    "   \n",
    "    data = data[['번호','명칭','요약', '출원인대표명', '출원인국가',\n",
    "                 '국제특허분류', '공통특허분류', '미국특허분류', '출원일','INPADOC 패밀리',\n",
    "                 '독립 청구항수',\n",
    "                 '전체 청구항수', '대표 청구항', '전체 청구항', '권리 현황', '최종 상태',\n",
    "                 '자국인용특허', '외국인용특허', '자국인용횟수', '자국피인용횟수' ,'INPADOC패밀리수',\n",
    "                 'INPADOC패밀리국가수', '발명자수', '소유권이전여부','file_name']]\n",
    "\n",
    "    data.columns = ['pt_id', 'title', 'abstract', 'applicant', 'country',\n",
    "                    'IPC', 'CPC', 'USPC', 'application_date', 'family_pat',\n",
    "                    'ind_claims_cnt',\n",
    "                    'total_claims_cnt','claims_rep','claims', 'right_status', 'final_status',\n",
    "                    'cited_pat_in', 'cited_pat_out', 'forward_cited_in_cnt', 'backward_cited_in_cnt', 'family_cnt',\n",
    "                    'family_country_cnt','inventor_cnt','transfered','file_name']    \n",
    "   \n",
    "    data = data.dropna(subset = ['application_date']).reset_index(drop = 1)\n",
    "    data['application_year'] = data['application_date'].apply(lambda x : int(x.split('.')[0]))\n",
    "    data['TA'] = data['title'] + ' ' + data['abstract']\n",
    "    data['TAF'] = data['title'] + ' ' + data['abstract'] + ' ' + data['claims_rep']\n",
    "   \n",
    "    # data['TAF'] = data.apply(lambda x: x.TA +' '+ x.claims_rep if str(x.claims_rep) != 'nan' else x.TA, axis= 1)\n",
    "   \n",
    "    # 1. 패밀리 특허 제거\n",
    "    # for idx, row in data.iterrows() :\n",
    "       \n",
    "    #     code = row['pt_id'][0:2]\n",
    "    #     fam_list = row['family_pat'].split(', ')\n",
    "    #     fam_list = [i for i in fam_list if i[0:2] == code]\n",
    "    #     print(fam_list)\n",
    "    #     for idx_, row_ in data.iterrows() :\n",
    "    #         if idx != idx_ :\n",
    "    #             pt_id_ = row['pt_id']\n",
    "    #             if pt_id_ in fam_list :\n",
    "    #                 data = data.drop(idx_)\n",
    "    #                 print(str(idx)+'의 패밀리특허 '+str(idx_)+'제거')\n",
    "   \n",
    "    # data = data.reset_index(drop = 1)\n",
    "   \n",
    "    # 2. 유사특허 필터링\n",
    "    # for idx, row in data.iterrows() :\n",
    "       \n",
    "    #     text = row['TA'][0:100]\n",
    "       \n",
    "    #     for idx_, row_ in data.iterrows() :\n",
    "    #         if idx != idx_ :\n",
    "    #             text_ = row_['TA'][0:100]\n",
    "    #             if lev(text, text_) <= 5 :\n",
    "    #                 data = data.drop(idx_)\n",
    "    #                 print(str(idx)+'과 유사한 '+str(idx_)+'제거')\n",
    "   \n",
    "   \n",
    "    return(data)\n",
    "\n",
    "    data = data.reset_index(drop = 1)\n",
    "   \n",
    "   \n",
    "   \n",
    "    #%%\n",
    "\n",
    "   \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    import os\n",
    "    import sys\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np    \n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    from datetime import timezone\n",
    "    import spacy\n",
    "\n",
    "    directory = os.path.dirname(os.path.abspath(__file__))\n",
    "    directory = directory.replace(\"\\\\\", \"/\") # window\n",
    "    os.chdir(directory)    \n",
    "    sys.path.append(directory+'/submodule')\n",
    "   \n",
    "    directory = 'D:/OneDrive - SNU/db/patent/Wisdomain/'\n",
    "    file_name = 'blockchain.csv'\n",
    "   \n",
    "    data = pd.read_csv(directory + file_name, skiprows=4)\n",
    "    data['file_name'] = file_name\n",
    "    data = wisdomain_prep(data)\n",
    "   \n",
    "   \n",
    "    #%% EDA\n",
    "   \n",
    "    from collections import Counter\n",
    "    import matplotlib.pyplot as plt\n",
    "   \n",
    "    c = Counter(data['application_year'])\n",
    "    c = sorted(c.items())\n",
    "   \n",
    "    plt.bar([i[0] for i in c], [i[1] for i in c])\n",
    "   \n",
    "   \n",
    "    #%%\n",
    "   \n",
    "    data_ = data.loc[data['application_year'] >= 2015, :].reset_index(drop = 1)\n",
    "    data_sample = data_.sample(1000, random_state = 1234).reset_index(drop = 1)\n",
    "   \n",
    "    #%% test\n",
    "   \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # doc = nlp(data_sample['TA'][0].lower())\n",
    "    data_sample['nlp'] = data_sample['TA'].apply(lambda x : nlp(x.lower()))\n",
    "    #%%\n",
    "    doc = data_sample['nlp'][0]\n",
    "   \n",
    "    for sent in doc.sents:\n",
    "        for token in sent :\n",
    "                print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "               \n",
    "       \n",
    "    #%%\n",
    "    # pos_data = [[[i.pos_ for i in sent] for sent in doc.sents] for doc in data_sample['nlp']]\n",
    "    # pos_data = [[[i.tag_ for i in sent] for sent in doc.sents] for doc in data_sample['nlp']]\n",
    "    # pos_data = [[[i.dep_ for i in sent] for sent in doc.sents] for doc in data_sample['nlp']]\n",
    "    pos_data = [[[i.lemma_ for i in sent] for sent in doc.sents] for doc in data_sample['nlp']]\n",
    "   \n",
    "    pos_data = sum(pos_data, [])\n",
    "    # pos_data = [[i.tag_ for i in doc] for doc in data_sample['nlp']]\n",
    "   \n",
    "    pos_data = [[i for i in doc if i != '.'] for doc in pos_data]\n",
    "    #%%\n",
    "    stopwords = ['.','the', 'a', 'to', 'of', 'and', ',', 'a', 'be', 'with']\n",
    "    pos_data = [[i for i in doc if i not in stopwords] for doc in pos_data]\n",
    "    # pos_data = [[i for i in doc if i != 'DT'] for doc in pos_data]\n",
    "    # pos_data = [[i for i in doc if i != 'punct'] for doc in pos_data]\n",
    "    # pos_data = [[i for i in doc if i != 'det'] for doc in pos_data]\n",
    "    # pos_data = [[i for i in doc if i != 'PUNCT'] for doc in pos_data]\n",
    "    # pos_data = [[i for i in doc if i != 'DET'] for doc in pos_data]\n",
    "   \n",
    "    #%%\n",
    "    from prefixspan import PrefixSpan\n",
    "   \n",
    "    ps = PrefixSpan(pos_data)\n",
    "    ps.maxlen = 4\n",
    "    ps.minlen = 2\n",
    "\n",
    "    result = ps.frequent(10)\n",
    "    # result = ps.frequent(100, closed = 1)\n",
    "    # result = ps.frequent(100, generator = 1)\n",
    "   \n",
    "   \n",
    "    result_ = []\n",
    "   \n",
    "    for tup in result :\n",
    "       \n",
    "        pat = tup[1]\n",
    "        test = 0\n",
    "        for index, item in enumerate(pat[:-1]) :\n",
    "            if item == pat[index+1] : test =1\n",
    "           \n",
    "        if test != 1 : result_.append(tup)\n",
    "           \n",
    "    #%%\n",
    "    # print(data_sample['TA'][0].lower())\n",
    "    chunk_list = []\n",
    "    for doc in data_sample['nlp'] :\n",
    "        # doc = data_sample['nlp'][0]\n",
    "        for chunk in doc.noun_chunks:\n",
    "            print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "                    chunk.root.head.text)\n",
    "            chunk_list.append(chunk.text)\n",
    "           \n",
    "    #%%\n",
    "    for pat in [r\"\\ba\\b\", r\"\\bthe\\b\", r\"\\ban\\b\"]:\n",
    "        chunk_list = [re.sub(pat,\"\",i) for i in chunk_list]\n",
    "    chunk_list = [i.strip() for i in chunk_list]\n",
    "    c= Counter(chunk_list)\n",
    "    c = pd.DataFrame(c.items())\n",
    "    # result = ps.topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
